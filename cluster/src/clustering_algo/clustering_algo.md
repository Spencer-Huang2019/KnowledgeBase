# 距离度量

## 明可夫斯基距离

$d(x_i, x_j) = ({|x_{i1} - x_{j1}|}^g + {|x_{i2} - x_{j2}|}^g + ... + {|x_{ip} - x_{jp}|}^g)^{1/g}$

## 欧式距离

两个点之间的直线距离。  

明可夫斯基距离中的  g=2 就是欧氏距离

## 曼哈顿距离

两个坐标轴的绝对距离之和，适合某些具有特定物理意义的数据

## 余弦相似度

用于文本数据和高纬度稀疏数据，通过余弦值衡量向量间的相似性

$\cos(\theta) = \frac{x_i^T \cdot x_j}{||x_i|| \cdot ||x_j||}$

# 聚类算法

以下算法都用 C++ 进行实现，有些代码是参考的大模型给的结果，不过在实际使用中也遇到些问题，然后做了一些改进。

## k-means

以空间中 k 个点为中心进行聚类，对最靠近中心的对象归类。逐次计算各簇中心的值为新的中心值，迭代更新，直至得到最好的聚类结果.

目标函数：各簇成员到其聚类中心的距离的平方和最小

![img](https://tanway-admin.feishu.cn/space/api/box/stream/download/asynccode/?code=ZTQzMWNlMjNmOGY4NTBjNzU4NTUyZDdlYjgxOTgzOTVfVjRrc1ZCZjR6VmVWVkFhMTJKc2VwNFM5RGV1T3hndkxfVG9rZW46T2hqT2JnVkJ2b1M2QXl4cWFRWmNHQkRqbkNoXzE3NDE1Njg2NDU6MTc0MTU3MjI0NV9WNA)

#### 算法流程

1. 选择 k 个类的初始中心
2. 遍历每个点，计算点都中心 c 的距离，将点归类到距离最小的 c 对应的簇
3. 更新每个类的中心点
4. 如果类的中心点保持不变，那么迭代结束

#### 适用场景

服从高斯分布的样本数据集的划分

#### 缺点

需要事先确定 K 值；采用迭代方法，只能得到局部最优；对初始值的选取比较敏感；算法时间开销大

## DBSCAN

基于一组邻域来描述样本集的紧密程度，参数 ($\varepsilon,MinPts$) 用来描述邻域的样本分布紧密程度。$\varepsilon$描述了某一样本的邻域距离阈值，$MinPts$描述了某一样本的距离为 $\varepsilon$的邻域中样本个数的阈值。

定义：由密度可达关系导出的最大密度相连的样本集合，即为最终聚类的一个类别，或者说一个簇。

样本点组成：核心点（距离为 $\varepsilon$的邻域内至少有 $MinPts$ 个点时），边界点（在某个核心点的邻域内，但是其$\varepsilon$邻域内的数量少于  $MinPts$ ），噪声点（非核心点，且不在任意的核心邻域内）

原理：只要任意两个样本点是密度直达或密度可达的关系，那么该两个样本点归为同一簇类

### DBSCAN

#### 算法流程

1. 遍历数据集，计算每个样本点在$\varepsilon$的邻域内的样本数，如果邻域内样本数小于$MinPts$记为噪声
2. 如果为核心点，那么增加簇类的个数，且初始化这个核心点的簇类
3. 遍历该核心点的所有邻域样本，标记这些邻域样本的簇类，计算邻域样本的邻域，如果是核心样本，那么添加到种子集（该步中邻域样本集）

$MinPts$**的选择**

通常 $MinPts$ 大于等于数据集的维度 D + 1。若数据集越大，则 minPts 的值选择的亦越大。

$\varepsilon$**的选择**

计算每个样本与所有样本的距离，选择第 k 个最近邻的距离，并从大到小排序得到 k 距离曲线，曲线拐点对应的距离设置为 $\varepsilon$.

#### 优点

1. 可以对任意形状的稠密数据集进行聚类
2. 可以在聚类的同时发现异常点，对数据集中的异常点不敏感
3. 聚类结果没有偏倚

#### 缺点

1. 如果样本集的密度不均匀、聚类间距差相差很大时，DBSCAN 聚类不适合
2. 样本集大，聚类收敛时间长
3. 需要对 $\varepsilon,MinPts$进行联合调参

### 自适应 DBSCAN

普通的 DBSCAN 的$\varepsilon$是固定的不变的。在实际的应用中，比如说激光雷达点云，远处的物体的点云通常会变得比较稀疏，物体的点云形态也通常不完整，如果使用相同的 $\varepsilon$，那么远处的同一个物体很多时候会被聚成多个簇。







